{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ffd791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding,\n",
    "    LSTM,\n",
    "    Bidirectional,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    SpatialDropout1D,\n",
    "    BatchNormalization,\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dde0e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69426b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(\"../data/train_data.csv\")\n",
    "valid_df = pd.read_csv(\"../data/valid_data.csv\")\n",
    "\n",
    "train_df[\"text\"] = train_df[\"text\"].fillna(\"\")\n",
    "valid_df[\"text\"] = valid_df[\"text\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b958798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning \n",
    "def clean_financial_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    # URLs\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+|https\\S+\", \"\", text)\n",
    "\n",
    "    # Mentions\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "\n",
    "    # HTML entities\n",
    "    text = re.sub(r\"&\\w+;\", \" \", text)\n",
    "\n",
    "    # Keep letters, numbers, spaces, $, %, dots, commas\n",
    "    text = re.sub(r\"[^a-z0-9\\s\\$\\%\\.\\,]\", \" \", text)\n",
    "\n",
    "    # Normalize stock tickers: \"$ aapl\" -> \"$aapl\"\n",
    "    text = re.sub(r\"\\$\\s+([a-z]+)\", r\"$\\1\", text)\n",
    "\n",
    "    # Normalize percentages: \"10  %\" -> \"10%\"\n",
    "    text = re.sub(r\"(\\d+)\\s*%\", r\"\\1%\", text)\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4721624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "train_df[\"clean_text\"] = train_df[\"text\"].apply(clean_financial_text)\n",
    "valid_df[\"clean_text\"] = valid_df[\"text\"].apply(clean_financial_text)\n",
    "\n",
    "# Drop any empty texts\n",
    "train_df = train_df[train_df[\"clean_text\"].str.len() > 0].reset_index(drop=True)\n",
    "valid_df = valid_df[valid_df[\"clean_text\"].str.len() > 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e044334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "MAX_SEQ_LENGTH = 50\n",
    "EMBEDDING_DIM = 128\n",
    "OOV_TOKEN = \"<OOV>\"\n",
    "\n",
    "# Same custom filters as original\n",
    "custom_filters = '!\"#&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=MAX_VOCAB_SIZE,\n",
    "    oov_token=OOV_TOKEN,\n",
    "    filters=custom_filters,\n",
    ")\n",
    "\n",
    "tokenizer.fit_on_texts(train_df[\"clean_text\"])\n",
    "vocab_size = min(len(tokenizer.word_index) + 1, MAX_VOCAB_SIZE)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_df[\"clean_text\"])\n",
    "X_valid_seq = tokenizer.texts_to_sequences(valid_df[\"clean_text\"])\n",
    "\n",
    "X_train = pad_sequences(\n",
    "    X_train_seq, maxlen=MAX_SEQ_LENGTH, padding=\"post\", truncating=\"post\"\n",
    ")\n",
    "X_valid = pad_sequences(\n",
    "    X_valid_seq, maxlen=MAX_SEQ_LENGTH, padding=\"post\", truncating=\"post\"\n",
    ")\n",
    "\n",
    "y_train = train_df[\"label\"].values\n",
    "y_valid = valid_df[\"label\"].values\n",
    "\n",
    "NUM_CLASSES = 20  # same as original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ded9bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class weights\n",
    "class_weights_array = class_weight.compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train,\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights_array))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7d9166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiLSTM model\n",
    "def build_bilstm_model(vocab_size, embedding_dim, max_length, num_classes):\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Embedding(\n",
    "                input_dim=vocab_size,\n",
    "                output_dim=embedding_dim,\n",
    "                input_length=max_length,\n",
    "                name=\"embedding\",\n",
    "            ),\n",
    "            SpatialDropout1D(0.3, name=\"spatial_dropout\"),\n",
    "            Bidirectional(\n",
    "                LSTM(\n",
    "                    128,\n",
    "                    return_sequences=False,\n",
    "                    dropout=0.2,\n",
    "                    recurrent_dropout=0.2,\n",
    "                    kernel_regularizer=l2(0.001),\n",
    "                ),\n",
    "                name=\"bidirectional_lstm\",\n",
    "            ),\n",
    "            BatchNormalization(name=\"batch_norm_1\"),\n",
    "            Dense(128, activation=\"relu\", kernel_regularizer=l2(0.001), name=\"dense_1\"),\n",
    "            Dropout(0.5, name=\"dropout_1\"),\n",
    "            Dense(64, activation=\"relu\", kernel_regularizer=l2(0.001), name=\"dense_2\"),\n",
    "            Dropout(0.3, name=\"dropout_2\"),\n",
    "            Dense(num_classes, activation=\"softmax\", name=\"output\"),\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_bilstm_model(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    max_length=MAX_SEQ_LENGTH,\n",
    "    num_classes=NUM_CLASSES,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81142c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights_dict,\n",
    "    verbose=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30db8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions and metrics\n",
    "y_pred_probs = model.predict(X_valid, verbose=0)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_valid, y_pred)\n",
    "f1_macro = f1_score(y_valid, y_pred, average=\"macro\")\n",
    "f1_weighted = f1_score(y_valid, y_pred, average=\"weighted\")\n",
    "precision_macro = precision_score(y_valid, y_pred, average=\"macro\", zero_division=0)\n",
    "recall_macro = recall_score(y_valid, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(\"\\nEvaluation metrics:\")\n",
    "print(f\"  Accuracy:           {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"  F1 (macro):         {f1_macro:.4f}\")\n",
    "print(f\"  F1 (weighted):      {f1_weighted:.4f}\")\n",
    "print(f\"  Precision (macro):  {precision_macro:.4f}\")\n",
    "print(f\"  Recall (macro):     {recall_macro:.4f}\")\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_valid, y_pred, digits=4, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7ec663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix (raw + normalized)\n",
    "cm = confusion_matrix(y_valid, y_pred)\n",
    "cm_normalized = confusion_matrix(y_valid, y_pred, normalize=\"true\")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=False, cmap=\"Blues\", fmt=\"d\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix (BiLSTM)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_normalized, annot=False, cmap=\"Blues\", fmt=\".2f\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Normalized Confusion Matrix (BiLSTM)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a393d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class F1 bar chart (same logic)\n",
    "report_dict = classification_report(\n",
    "    y_valid, y_pred, output_dict=True, zero_division=0\n",
    ")\n",
    "\n",
    "f1_scores = [report_dict[str(i)][\"f1-score\"] for i in range(NUM_CLASSES)]\n",
    "supports = [report_dict[str(i)][\"support\"] for i in range(NUM_CLASSES)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "colors = [\n",
    "    \"green\" if f1 >= 0.5 else \"orange\" if f1 >= 0.3 else \"red\" for f1 in f1_scores\n",
    "]\n",
    "bars = ax.bar(range(NUM_CLASSES), f1_scores, color=colors, edgecolor=\"black\", alpha=0.8)\n",
    "\n",
    "for i, (bar, f1, sup) in enumerate(zip(bars, f1_scores, supports)):\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() + 0.01,\n",
    "        f\"{f1:.2f}\\n(n={sup})\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=8,\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Class\")\n",
    "ax.set_ylabel(\"F1 score\")\n",
    "ax.set_title(\"Per-class F1 scores\")\n",
    "ax.set_xticks(range(NUM_CLASSES))\n",
    "ax.set_xticklabels(range(NUM_CLASSES))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fb1d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"accuracy\"], label=\"train\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"val\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"loss\"], label=\"train\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a84cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['clean_text'].head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
